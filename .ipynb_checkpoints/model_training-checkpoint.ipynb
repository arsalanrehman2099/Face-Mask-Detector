{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caffe | MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = r\"./dataset/\"\n",
    "CATEGORIES = ['with_mask','without_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arsalan ur Rehman\\anaconda3\\lib\\site-packages\\PIL\\Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY,category)\n",
    "    \n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path,img)\n",
    "        image = load_img(img_path,target_size=(224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding - Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data,dtype='float32')\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20,\n",
    "                        zoom_range=0.15,\n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2,\n",
    "                        shear_range=0.15,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model (MobileNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 10s 1us/step\n"
     ]
    }
   ],
   "source": [
    "baseModel = MobileNetV2(weights='imagenet',include_top=False,\n",
    "                       input_tensor=Input(shape=(224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=baseModel.input, outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR,decay=INIT_LR/EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "95/95 [==============================] - 114s 1s/step - loss: 0.5867 - accuracy: 0.7075 - val_loss: 0.1017 - val_accuracy: 0.9765\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.1455 - accuracy: 0.9520 - val_loss: 0.0613 - val_accuracy: 0.9844\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 102s 1s/step - loss: 0.1149 - accuracy: 0.9626 - val_loss: 0.0485 - val_accuracy: 0.9857\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 93s 981ms/step - loss: 0.0613 - accuracy: 0.9774 - val_loss: 0.0504 - val_accuracy: 0.9857\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 91s 957ms/step - loss: 0.0600 - accuracy: 0.9800 - val_loss: 0.0383 - val_accuracy: 0.9896\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 92s 964ms/step - loss: 0.0552 - accuracy: 0.9809 - val_loss: 0.0332 - val_accuracy: 0.9896\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 92s 969ms/step - loss: 0.0656 - accuracy: 0.9776 - val_loss: 0.0310 - val_accuracy: 0.9909\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 96s 1s/step - loss: 0.0539 - accuracy: 0.9825 - val_loss: 0.0299 - val_accuracy: 0.9909\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 95s 1s/step - loss: 0.0459 - accuracy: 0.9808 - val_loss: 0.0337 - val_accuracy: 0.9883\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 0.0421 - accuracy: 0.9820 - val_loss: 0.0271 - val_accuracy: 0.9922\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 96s 1s/step - loss: 0.0425 - accuracy: 0.9852 - val_loss: 0.0282 - val_accuracy: 0.9922\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.0265 - val_accuracy: 0.9896\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0369 - accuracy: 0.9857 - val_loss: 0.0277 - val_accuracy: 0.9922\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0348 - accuracy: 0.9904 - val_loss: 0.0265 - val_accuracy: 0.9922\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 100s 1s/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.0268 - val_accuracy: 0.9896\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0231 - accuracy: 0.9945 - val_loss: 0.0253 - val_accuracy: 0.9896\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 96s 1s/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0231 - val_accuracy: 0.9909\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 112s 1s/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 0.0228 - val_accuracy: 0.9909\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 121s 1s/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.0202 - val_accuracy: 0.9948\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 114s 1s/step - loss: 0.0245 - accuracy: 0.9906 - val_loss: 0.0229 - val_accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(aug.flow(X_train,y_train,batch_size=BS),\n",
    "             steps_per_epoch=len(X_train) // BS,\n",
    "             validation_data=(X_test,y_test),\n",
    "             validation_steps=len(X_test) // BS,\n",
    "             epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIds = model.predict(X_test,batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIds = np.argmax(predIds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      1.00      0.99       383\n",
      "without_mask       1.00      0.99      0.99       384\n",
      "\n",
      "    accuracy                           0.99       767\n",
      "   macro avg       0.99      0.99      0.99       767\n",
      "weighted avg       0.99      0.99      0.99       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1),predIds,target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c6d3c02490>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix(y_test.argmax(axis=1),predIds),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mask_detector.model',save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

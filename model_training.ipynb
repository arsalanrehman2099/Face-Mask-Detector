{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet | Caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = r\"./dataset/\"\n",
    "CATEGORIES = ['with_mask','without_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arsalan ur Rehman\\anaconda3\\lib\\site-packages\\PIL\\Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY,category)\n",
    "    \n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path,img)\n",
    "        image = load_img(img_path,target_size=(224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding - Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data,dtype='float32')\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20,\n",
    "                        zoom_range=0.15,\n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2,\n",
    "                        shear_range=0.15,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model (MobileNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 10s 1us/step\n"
     ]
    }
   ],
   "source": [
    "baseModel = MobileNetV2(weights='imagenet',include_top=False,\n",
    "                       input_tensor=Input(shape=(224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=baseModel.input, outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR,decay=INIT_LR/EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "95/95 [==============================] - 114s 1s/step - loss: 0.5867 - accuracy: 0.7075 - val_loss: 0.1017 - val_accuracy: 0.9765\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.1455 - accuracy: 0.9520 - val_loss: 0.0613 - val_accuracy: 0.9844\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 102s 1s/step - loss: 0.1149 - accuracy: 0.9626 - val_loss: 0.0485 - val_accuracy: 0.9857\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 93s 981ms/step - loss: 0.0613 - accuracy: 0.9774 - val_loss: 0.0504 - val_accuracy: 0.9857\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 91s 957ms/step - loss: 0.0600 - accuracy: 0.9800 - val_loss: 0.0383 - val_accuracy: 0.9896\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 92s 964ms/step - loss: 0.0552 - accuracy: 0.9809 - val_loss: 0.0332 - val_accuracy: 0.9896\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 92s 969ms/step - loss: 0.0656 - accuracy: 0.9776 - val_loss: 0.0310 - val_accuracy: 0.9909\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 96s 1s/step - loss: 0.0539 - accuracy: 0.9825 - val_loss: 0.0299 - val_accuracy: 0.9909\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 95s 1s/step - loss: 0.0459 - accuracy: 0.9808 - val_loss: 0.0337 - val_accuracy: 0.9883\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 0.0421 - accuracy: 0.9820 - val_loss: 0.0271 - val_accuracy: 0.9922\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 96s 1s/step - loss: 0.0425 - accuracy: 0.9852 - val_loss: 0.0282 - val_accuracy: 0.9922\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.0265 - val_accuracy: 0.9896\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0369 - accuracy: 0.9857 - val_loss: 0.0277 - val_accuracy: 0.9922\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0348 - accuracy: 0.9904 - val_loss: 0.0265 - val_accuracy: 0.9922\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 100s 1s/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.0268 - val_accuracy: 0.9896\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0231 - accuracy: 0.9945 - val_loss: 0.0253 - val_accuracy: 0.9896\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 96s 1s/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0231 - val_accuracy: 0.9909\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 112s 1s/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 0.0228 - val_accuracy: 0.9909\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 121s 1s/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.0202 - val_accuracy: 0.9948\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 114s 1s/step - loss: 0.0245 - accuracy: 0.9906 - val_loss: 0.0229 - val_accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(aug.flow(X_train,y_train,batch_size=BS),\n",
    "             steps_per_epoch=len(X_train) // BS,\n",
    "             validation_data=(X_test,y_test),\n",
    "             validation_steps=len(X_test) // BS,\n",
    "             epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIds = model.predict(X_test,batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predIds = np.argmax(predIds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      1.00      0.99       383\n",
      "without_mask       1.00      0.99      0.99       384\n",
      "\n",
      "    accuracy                           0.99       767\n",
      "   macro avg       0.99      0.99      0.99       767\n",
      "weighted avg       0.99      0.99      0.99       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1),predIds,target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c6d5ad67f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix(y_test.argmax(axis=1),predIds),annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mask_detector.model',save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
